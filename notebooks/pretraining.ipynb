{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from regex_model_1 import RegexTokenizer\n",
    "tokenizer = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.load(model_file=\"/kaggle/input/tokenizer_medical/pytorch/default/1/tokenizer_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"I have fever what is a solution\", allowed_special='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer):\n",
    "    return len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "get_vocab_size(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoded_text_sequence = []\n",
    "# batch_size = 3_000_000\n",
    "# with open(\"/kaggle/input/text-med/text_medical.txt\", \"r\") as f:\n",
    "    \n",
    "#     while True:\n",
    "#         chunk = f.read(batch_size)\n",
    "#         if not chunk:\n",
    "#             break \n",
    "\n",
    "#         batch_tokens = tokenizer.encode(chunk, allowed_special=\"all\")\n",
    "#         encoded_text_sequence.extend(batch_tokens)\n",
    "#         print(f\"Processed {len(encoded_text_sequence)} tokens so far\")\n",
    "\n",
    "# print(f\"Total Tokens: {len(encoded_text_sequence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"/kaggle/working/encoded_text.npy\", \n",
    "#        np.array(encoded_text_sequence, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data = np.load(\"/kaggle/input/encoded-npy/encoded_text.npy\",\n",
    "#               mmap_mode='r')\n",
    "\n",
    "# print(\"Shape of data:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the formatted dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "conversation = []\n",
    "with open(\"/kaggle/input/foramatted/formmated_dataset.jsonl\", \"r\",\n",
    "         encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        conv = json.loads(line)\n",
    "        conversation.append(conv[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(conversation[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the text using tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_text = []\n",
    "\n",
    "for sample in conversation:\n",
    "    tokens = tokenizer.encode(sample, allowed_special=\"all\")\n",
    "    encoded_text.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encoded_text, dtype=torch.long)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "num_blocks = len(data) // block_size\n",
    "data = data[:num_blocks*block_size].view(-1, block_size)\n",
    "print(\"Shape of data\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset Class for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FineTunedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data: torch.Tensor, \n",
    "                 padding_token: int, device:str\n",
    "                ):\n",
    "        \n",
    "        self.data = data\n",
    "        self.padding_token = padding_token\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        sample = self.data[index]\n",
    "        x = sample.to(self.device)\n",
    "        y = sample[1:].to(self.device)\n",
    "        padding_tensor = torch.tensor([self.padding_token], device=self.device)\n",
    "        y = torch.cat((y, padding_tensor))\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_split = int(0.95*len(data))\n",
    "train_data_split = data[:train_split]\n",
    "val_data_split = data[train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataloaders for training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "padding_token = tokenizer.special_tokens[\"<|PAD|>\"]\n",
    "\n",
    "\n",
    "train_dataset = FineTunedDataset(data=train_data_split,\n",
    "                               padding_token=padding_token,\n",
    "                                device=device\n",
    "                                )\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True\n",
    "                             )\n",
    "\n",
    "val_dataset = FineTunedDataset(data=val_data_split,\n",
    "                               padding_token=padding_token,\n",
    "                               device=device\n",
    "                               )\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from GPTmodel import GPTLanguageModel\n",
    "\n",
    "block_size= 1024\n",
    "n_embedding = 384\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "padding_token = 3077\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embedding=n_embedding,\n",
    "    n_head=n_head, block_size=block_size,\n",
    "    n_layer=n_layer, dropout=dropout, \n",
    "    padding_token=padding_token, device=device)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in model.parameters())/1e6,\"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_batch(split: str, split_index: int,\n",
    "#              block_size: int, device:str,\n",
    "#              data):\n",
    "\n",
    "#     if split == \"train\":\n",
    "#         start_index = 0\n",
    "#         end_index = split_index\n",
    "        \n",
    "#     else:\n",
    "#         start_index = split_index\n",
    "#         end_index = len(data)\n",
    "\n",
    "#     available_blocks = (end_index - start_index - 1) // block_size\n",
    "#     block_indices = torch.randint(0, available_blocks, (batch_size,))\n",
    "\n",
    "#     x_batch, y_batch = [], []\n",
    "#     for i in block_indices:\n",
    "#         block_start = start_index + (i * block_size)\n",
    "#         x_batch.append(data[block_start:block_start+block_size])\n",
    "#         y_batch.append(data[block_start+1:block_start+block_size+1])\n",
    "\n",
    "#     x_batch = np.array(x_batch)\n",
    "#     y_batch = np.array(y_batch)\n",
    "\n",
    "#     x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "#     y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "#     return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate loss function\n",
    "* to estimate the loss of training and validation splits\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def estimate_loss(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader) -> Dict[str, float]:\n",
    "\n",
    "  total_loss = {}\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  model.eval()\n",
    "    \n",
    "  for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "    losses = torch.zeros(len(loader))\n",
    "\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "      with torch.inference_mode():\n",
    "       _, loss = model(x, y)\n",
    "      #  print(_.shape)\n",
    "      losses[i] = loss.item()\n",
    "    total_loss[split] = losses.mean().item()\n",
    "\n",
    "  model.train()\n",
    "  return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model: GPTLanguageModel,\n",
    "                   optimizer: torch.optim.Optimizer,\n",
    "                   epoch: int, loss: float,\n",
    "                   file_path: str,global_step: int,\n",
    "                   scheduler) -> None:\n",
    "\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"loss\": loss, \n",
    "        \"global_step\": global_step\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_checkpoint\",\n",
    "                       map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainining Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "max_iters = 10\n",
    "eval_interval = len(train_dataloader) // 15\n",
    "learning_rate = 1e-5\n",
    "warmup_steps=100\n",
    "clip_grad_norm = 1.0\n",
    "global_step = 0\n",
    "start_epoch = 0\n",
    "gradient_accumulation_step = 8\n",
    "weight_decay = 0.01\n",
    "total_steps = len(train_dataloader) * max_iters // gradient_accumulation_step\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                              weight_decay=0.01\n",
    "                             )\n",
    "\n",
    "if checkpoint:\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps\n",
    "                                            )\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "    global_step = checkpoint.get(\"global_step\", 0)\n",
    "    start_epoch = checkpoint.get(\"epoch\", 0) + 1\n",
    "    \n",
    "train_loss = []\n",
    "val_loss = []\n",
    "total_lrs = []\n",
    "trained_loss = []\n",
    "\n",
    "for epoch in range(start_epoch, max_iters):\n",
    "    model.train()\n",
    "    train_loss_backprop = 0.0\n",
    "    epoch_lrs = []\n",
    "    \n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(\n",
    "        iterable=enumerate(train_dataloader),desc='Training on batches',\n",
    "        total=len(train_dataloader)):\n",
    "        \n",
    "        if global_step % eval_interval == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "            losses = estimate_loss(model=model,\n",
    "                             train_loader=train_dataloader,\n",
    "                             val_loader=val_dataloader)\n",
    "            \n",
    "            print(f\"Iteration: {epoch}/step {batch_idx} |\"\n",
    "                  f\"Train Loss: {losses['train']:.4f} |\"\n",
    "                  f\"Validation Loss: {losses['val']:.4f}\")\n",
    "            \n",
    "            train_loss.append(losses['train'])\n",
    "            val_loss.append(losses['val'])\n",
    "\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        if batch_idx % gradient_accumulation_step == 0:\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), \n",
    "                                           max_norm=clip_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        global_step += 1\n",
    "        \n",
    "        train_loss_backprop += loss.item()\n",
    "        trained_loss.append(loss.item())\n",
    "\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        epoch_lrs.append(current_lr)\n",
    "        \n",
    "    avg_epoch_loss = train_loss_backprop / (batch_idx+1)      \n",
    "    print(f'\\nEpoch {epoch} average train loss:{avg_epoch_loss:.4f}\\n')\n",
    "\n",
    " \n",
    "    save_checkpoint(model=model,optimizer=optimizer,\n",
    "                  epoch=epoch,loss=avg_epoch_loss,\n",
    "                  file_path=f\"/kaggle/working/pretrained_checkponint_{epoch}.pth\",\n",
    "                   global_step=global_step, scheduler=scheduler)\n",
    "\n",
    "    total_lrs.extend(epoch_lrs)\n",
    "\n",
    "\n",
    "if global_step % gradient_accumulation_step != 0:\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_loss, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\", marker=\"x\")\n",
    "plt.title(\"Training Loss x Validation Loss\")\n",
    "plt.xlabel(\"Intervals\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig(f\"loss_plot_pretrained_{epoch}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(total_lrs, label=\"Learning rate\", marker=\"o\")\n",
    "plt.title(\"LR Curve\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"lr\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig(f\"lr_plot_pretrained_{epoch}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"A 33-year-old woman is brought to the emergency department 15 minutes after being stabbed in the chest with a screwdriver. Given her vital signs of pulse 110min, respirations 22min, and blood pressure 9065 mm Hg, along with the presence of a 5-cm deep stab wound at the upper border of the 8th rib in the left midaxillary line, which anatomical structure in her chest is most likely to be injured?\"\n",
    "input_ids = tokenizer.encode(prompt, allowed_special='all')\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    output = model.generate(input_ids, 200,\n",
    "                           block_size, 0.7,\n",
    "                           top_k=50, top_p=None)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_input_tokens(turns: list[dict]) -> list[int]:\n",
    "\n",
    "    formatted_input = \"\"\n",
    "    for turn in turns:\n",
    "        role = turn['role']\n",
    "        content = turn['content']\n",
    "        formatted_input += f\"<|startoftext|><|User|>{content}\"\n",
    "\n",
    "    formatted_input += f\"|Assistant|>\"\n",
    "    \n",
    "    input_tokens = tokenizer.encode(formatted_input, allowed_special='all')\n",
    "    input_tokens = torch.tensor(input_tokens, dtype=torch.long)\n",
    "    input_tokens = input_tokens.unsqueeze(0).to(device)\n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "def generate_message(input_tokens: list[int]):\n",
    "    model_answer = \"\"\n",
    "    \n",
    "    model.eval()\n",
    "    while True:\n",
    "    \n",
    "        try:\n",
    "            output_tokens = model.generate(\n",
    "                input_tokens=input_tokens,max_new_tokens=1,\n",
    "                block_size=1024, top_k=50, top_p=None,\n",
    "                temperature=0.7\n",
    "            )\n",
    "    \n",
    "            last_generated_tokens = output_tokens[0, -1].item()\n",
    "            \n",
    "            if last_generated_tokens == tokenizer.special_tokens['<|endoftext|>']:\n",
    "                break\n",
    "    \n",
    "    \n",
    "            input_tokens = torch.cat((input_tokens, output_tokens[:, -1:]), dim=1)\n",
    "            model_answer += tokenizer.decode([last_generated_tokens])\n",
    "    \n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return model_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_msg = \"What is cause of urine loss?\"\n",
    "turns = [{\n",
    "    \"role\": 'user',\n",
    "    \"content\": user_msg\n",
    "}]\n",
    "\n",
    "input_tokens = get_input_tokens(turns)\n",
    "model_answer = generate_message(input_tokens)\n",
    "\n",
    "turns.append({\n",
    "    \"role\": 'assistant',\n",
    "    \"content\": model_answer\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for turn in turns:\n",
    "    role = turn['role']\n",
    "    if role == 'user':\n",
    "        print(\"User:\", turn['content'] + \"\\n\")\n",
    "\n",
    "    elif role == 'assistant':\n",
    "        print(\"Assistant:\", turn['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8030714,
     "sourceId": 12706700,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8030718,
     "sourceId": 12706704,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 423318,
     "modelInstanceId": 405408,
     "sourceId": 512164,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 423326,
     "modelInstanceId": 405415,
     "sourceId": 512173,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 424106,
     "modelInstanceId": 406183,
     "sourceId": 513549,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 424316,
     "modelInstanceId": 406397,
     "sourceId": 513950,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
