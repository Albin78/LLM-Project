{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing tokenizer and loading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.tokenizer.regex_tokenizer import RegexTokenizer\n",
    "tokenizer = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.load(model_file=r\"src\\tokenizer\\tokenizer_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer):\n",
    "\n",
    "    vocab = tokenizer.vocab\n",
    "    return len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "get_vocab_size(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "dataset = []\n",
    "with open(\"datasets\\fine_tuned_dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "   for line in f:\n",
    "       dataset.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if the block size is not exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_tokens = 0\n",
    "block_size = 1024\n",
    "\n",
    "for data in dataset:\n",
    "    concat_msg = \"\"\n",
    "    for msg in data:\n",
    "        content = msg['content']\n",
    "        concat_msg += content + \"\\n\"\n",
    "        \n",
    "    tokens = tokenizer.encode(concat_msg)\n",
    "    max_tokens = max(max_tokens, len(tokens))\n",
    "\n",
    "    if len(tokens) > block_size:\n",
    "        print(f\"The tokens which exceed block size with length: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating encoded data for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_format_text(data: list[list[dict]],\n",
    "                      block_size: int,\n",
    "                     tokenizer):\n",
    "\n",
    "    fine_tuned_data = []\n",
    "\n",
    "    for conversation in data:\n",
    "        concat_msg = \"<|startoftext|>\"\n",
    "        for message in conversation:\n",
    "            role = message[\"role\"]\n",
    "            content = re.sub(r\"[^\\w\\s.,?<>|]\", \"\", message['content']).strip()\n",
    "            # content = remove_persona(content)\n",
    "\n",
    "            if role == \"user\":\n",
    "                concat_msg += f\"<|User|>{content}\"\n",
    "\n",
    "            else:\n",
    "                concat_msg += f\"<|Assistant|>{content}\"\n",
    "                sample = concat_msg + \"<|endoftext|>\"\n",
    "\n",
    "                encoded_msg = tokenizer.encode(sample,\n",
    "                                              allowed_special=\"all\")\n",
    "                \n",
    "                if len(encoded_msg) <= block_size:\n",
    "                    fine_tuned_data.append(encoded_msg)\n",
    "\n",
    "    return fine_tuned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fine_tuned = clean_format_text(data=dataset,\n",
    "                               block_size=1024,\n",
    "                               tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Total data for fine tuning:\", len(fine_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(fine_tuned[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the max length and min length tokens from the encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 0\n",
    "min_token = len(fine_tuned[0])\n",
    "for i in fine_tuned:\n",
    "    max_sequence_length = max(max_sequence_length, len(i))\n",
    "    min_token = min(min_token, len(i))\n",
    "\n",
    "print(\"Max length tokens:\", max_sequence_length)\n",
    "print(\"Minimum length tokens:\", min_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying padding to make the shapes as same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from typing import Tuple, Dict\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_pad(data: list[list[int]], max_sequence_length: int,\n",
    "             padding_token: int) -> torch.Tensor:\n",
    "\n",
    "    tensors = []\n",
    "    for i in range(len(data)):\n",
    "        tensor = torch.tensor(data[i])\n",
    "        padded_tensor = F.pad(\n",
    "            input=tensor,\n",
    "            pad=(0, block_size - len(tensor)),\n",
    "            value=padding_token\n",
    "        )\n",
    "        tensors.append(padded_tensor)\n",
    "\n",
    "    return torch.stack(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_tensor = apply_pad(data=fine_tuned,\n",
    "                             max_sequence_length=1024,\n",
    "                             padding_token=3077)\n",
    "print(\"The shape of train data shape:\", train_data_tensor.shape, \"\\n\")\n",
    "print(\"\\n Train data tensor padded:\", train_data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "split_index = int(0.95 * len(train_data_tensor)) \n",
    "train_data_split = train_data_tensor[:split_index]\n",
    "val_data_split = train_data_tensor[split_index:]\n",
    "\n",
    "print(\"Train data split shape:\", train_data_split.shape)\n",
    "print(\"Validation data split shape:\", val_data_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FineTunedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, padding_token: int, \n",
    "                 data: torch.Tensor, device: str,\n",
    "                 tokenizer, assist_token: int,\n",
    "                 special_tokens\n",
    "                ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        self.data = data\n",
    "        self.padding_token = padding_token\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.special_tokens = special_tokens\n",
    "        self.assist_token = assist_token\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "       sample = self.data[index]\n",
    "       x = sample.to(self.device)\n",
    "       y = sample[1:].to(self.device)\n",
    "       padded_tensor = torch.tensor([self.padding_token], device=self.device)\n",
    "       y = torch.cat((y, padded_tensor))\n",
    "       masked_y = self._masked(x, y)\n",
    "\n",
    "       return x, masked_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def _masked(self, x: torch.Tensor,\n",
    "               y: torch.Tensor):\n",
    "\n",
    "        mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "        special_token_tensor = torch.tensor(self.special_tokens, device=self.device)\n",
    "        mask |= torch.isin(y, special_token_tensor)\n",
    "\n",
    "        try:\n",
    "            assist_pos = (x == self.assist_token).nonzero(as_tuple=True)[0].item()\n",
    "            mask[:assist_pos+1] = True\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        y[mask] = self.padding_token\n",
    "        return y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "special_tokens = [tokenizer.encode(tok, allowed_special='all')[0] for tok in tokenizer.special_tokens\n",
    "                 if tok not in [\"<|endoftext|>\", \"<|PAD|>\"]]\n",
    "assist_token = tokenizer.encode('<|Assistant|>', allowed_special='all')\n",
    "\n",
    "\n",
    "train_dataset = FineTunedDataset(data=train_data_split,\n",
    "                                padding_token=3077,\n",
    "                                device=device,\n",
    "                                tokenizer=tokenizer,\n",
    "                                assist_token=assist_token,\n",
    "                                special_tokens=special_tokens\n",
    "                                )\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "val_dataset = FineTunedDataset(data=val_data_split,\n",
    "                              padding_token=3077,\n",
    "                              device=device,\n",
    "                              tokenizer=tokenizer,\n",
    "                              assist_token=assist_token,\n",
    "                              special_tokens=special_tokens\n",
    "                              )\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.special_tokens['<|endoftext|>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the model with hyperparameters\n",
    "\n",
    "model_id: 1KudWncwbEhANs3_WU2Jrk5IeoKYxrCpH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.model.GPTModel import GPTLanguageModel\n",
    "\n",
    "block_size= 1024\n",
    "n_embedding = 384\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "padding_token = 3077\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embedding=n_embedding,\n",
    "    n_head=n_head, block_size=block_size,\n",
    "    n_layer=n_layer, dropout=dropout, \n",
    "    padding_token=padding_token, device=device)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Model has\",sum(p.numel() for p in model.parameters())/1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def estimate_loss(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader) -> Dict[str, float]:\n",
    "\n",
    "  total_loss = {}\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  model.eval()\n",
    "    \n",
    "  for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "    losses = torch.zeros(len(loader))\n",
    "\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "      with torch.inference_mode():\n",
    "       _, loss = model(x, y)\n",
    "      #  print(_.shape)\n",
    "      losses[i] = loss.item()\n",
    "    total_loss[split] = losses.mean().item()\n",
    "\n",
    "  model.train()\n",
    "  return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model: GPTLanguageModel,\n",
    "                   optimizer: torch.optim.Optimizer,\n",
    "                   epoch: int, loss: float,\n",
    "                   file_path: str,global_step: int=None,\n",
    "                   scheduler=None) -> None:\n",
    "\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr_old = 1e-5\n",
    "new_batch = 16\n",
    "old_batch = 32\n",
    "\n",
    "lr_new = lr_old * (new_batch/old_batch)\n",
    "lr_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "max_iters = 10\n",
    "learning_rate = 5e-6\n",
    "weight_decay = 0.01\n",
    "# total_steps = len(train_loader) * max_iters\n",
    "# warmup_steps = int(0.03 * total_steps)\n",
    "eval_interval = len(train_loader) // 5\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"/kaggle/input/model_fine_8/pytorch/default/1/fine_tuned_checkpoint8 (1).pth\",\n",
    "                       map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Preatrained Model loaded successfully!.....\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                              weight_decay=weight_decay\n",
    "                             )\n",
    "\n",
    "start_epoch = checkpoint.get(\"epoch\", 0) + 1\n",
    "    \n",
    "train_loss = []\n",
    "val_loss = []\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(start_epoch, max_iters):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(\n",
    "        iterable=enumerate(train_loader),desc=f\"Epoch {epoch+1}/{max_iters}\",\n",
    "        total=len(train_loader)): \n",
    "\n",
    "        if global_step % eval_interval == 0 and global_step > 0:\n",
    "                model.eval()\n",
    "                losses = estimate_loss(model=model,\n",
    "                             train_loader=train_loader,\n",
    "                             val_loader=val_loader)\n",
    "            \n",
    "                print(f\"step {global_step} |\"\n",
    "                      f\"Train Loss: {losses['train']:.4f} |\"\n",
    "                      f\"Validation Loss: {losses['val']:.4f}\")\n",
    "            \n",
    "                train_loss.append(losses['train'])\n",
    "                val_loss.append(losses['val'])\n",
    "                model.train()\n",
    "        \n",
    "\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() \n",
    "        num_batches += 1\n",
    "        global_step += 1\n",
    "        \n",
    "            \n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    epoch_losses.append(avg_epoch_loss)\n",
    "        \n",
    "    print(f\"\\Iteration: {epoch+1} completed | average train loss:{avg_epoch_loss:.4f}\\n\")\n",
    "\n",
    " \n",
    "    save_checkpoint(model=model,optimizer=optimizer,\n",
    "                    epoch=epoch,loss=avg_epoch_loss,\n",
    "                    file_path=f\"/kaggle/working/fine_tuned_checkpoint{epoch+1}.pth\"\n",
    "                   )\n",
    "\n",
    "\n",
    "print(\"Fine tuning completed!.......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "eval_steps = np.arange(len(train_loss)) * eval_interval\n",
    "plt.plot(eval_steps, train_loss, 'b-',  label=\"Train Loss\", linewidth=2)\n",
    "plt.plot(eval_steps, val_loss, 'r-', label=\"Validation Loss\", linewidth=2)\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"loss_fine_tuned{epoch+1}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting epoch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if epoch_losses:\n",
    "    plt.plot(range(1, len(epoch_losses)+1), epoch_losses, color='purple', \n",
    "             marker='o', linewidth=2, markersize=6)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    plt.title(\"Average Loss per Epoch\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"/kaggle/working/average_loss_fine_tuned_{epoch+1}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(\"/kaggle/input/check_10/pytorch/default/1/fine_tuned_checkpoint10 (1).pth\",\n",
    "                       map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def order_response(text: str) -> str:\n",
    "    text = re.sub(r\"^[A-D]\\.\\s*\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    output = model.generate(input_ids, 200,\n",
    "                           block_size, 0.8,\n",
    "                           top_k=50, top_p=0.9)\n",
    "output = output.squeeze().tolist()\n",
    "response_tokens = output[input_ids.shape[1]:]\n",
    "response = tokenizer.decode(response_tokens)\n",
    "response = order_response(response)\n",
    "print(\"User:\", prompt, \"\\n\")\n",
    "print(\"Assistant:\", response.replace(\"<|endoftext|>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_input_tokens(turns: list[dict]) -> list[int]:\n",
    "\n",
    "    formatted_input = \"\"\n",
    "    for turn in turns:\n",
    "        role = turn['role']\n",
    "        content = turn['content']\n",
    "        formatted_input += f\"<|startoftext|><|User|>{content}\"\n",
    "\n",
    "    formatted_input += f\"|Assistant|>\"\n",
    "    \n",
    "    input_tokens = tokenizer.encode(formatted_input, allowed_special='all')\n",
    "    input_tokens = torch.tensor(input_tokens, dtype=torch.long)\n",
    "    input_tokens = input_tokens.unsqueeze(0).to(device)\n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "def generate_message(input_tokens: list[int]):\n",
    "    model_answer = \"\"\n",
    "    \n",
    "    model.eval()\n",
    "    while True:\n",
    "    \n",
    "        try:\n",
    "            output_tokens = model.generate(\n",
    "                input_tokens=input_tokens,max_new_tokens=1,\n",
    "                block_size=1024, top_k=50, top_p=0.9,\n",
    "                temperature=0.9\n",
    "            )\n",
    "    \n",
    "            last_generated_tokens = output_tokens[0, -1].item()\n",
    "            \n",
    "            if last_generated_tokens == tokenizer.special_tokens['<|endoftext|>']:\n",
    "                break\n",
    "    \n",
    "    \n",
    "            input_tokens = torch.cat((input_tokens, output_tokens[:, -1:]), dim=1)\n",
    "            model_answer += tokenizer.decode([last_generated_tokens])\n",
    "    \n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "        model_answer = order_response(model_answer)\n",
    "\n",
    "    return model_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_msg = \"Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\"\n",
    "turns = [{\n",
    "    \"role\": 'user',\n",
    "    \"content\": user_msg\n",
    "}]\n",
    "\n",
    "input_tokens = get_input_tokens(turns)\n",
    "model_answer = generate_message(input_tokens)\n",
    "\n",
    "turns.append({\n",
    "    \"role\": 'assistant',\n",
    "    \"content\": model_answer\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for turn in turns:\n",
    "    role = turn['role']\n",
    "    if role == 'user':\n",
    "        print(\"User:\", turn['content'] + \"\\n\")\n",
    "\n",
    "    elif role == 'assistant':\n",
    "        print(\"Assistant:\", turn['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8067725,
     "sourceId": 12762156,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 427732,
     "modelInstanceId": 409884,
     "sourceId": 521195,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 428385,
     "modelInstanceId": 410539,
     "sourceId": 522224,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 429292,
     "modelInstanceId": 411475,
     "sourceId": 523958,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 429353,
     "modelInstanceId": 411535,
     "sourceId": 524076,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
