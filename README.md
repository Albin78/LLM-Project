ğŸ¥ Medical Chat Assistant (Custom GPT + RAG)

A domain-specific medical chat assistant built with a custom GPT-style LLM and Retrieval-Augmented Generation (RAG).
The assistant is designed to handle medical queries by combining generative reasoning with factual retrieval from a medical knowledge base.

âœ¨ Features

ğŸ§  Custom GPT-style Transformer trained with attention mechanism.

ğŸ”¡ Custom Tokenizer for encoding/decoding medical text.

ğŸ“š RAG Pipeline using FAISS for document retrieval.

âš¡ FastAPI Backend exposing inference and retrieval endpoints.

ğŸ¨ Streamlit Frontend for user-friendly chat interface.

ğŸ³ Dockerized Deployment (FastAPI + Streamlit with Docker Compose).


ğŸ“‚ Project Structure
LLM-Project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                 # FastAPI backend
â”‚   â”œâ”€â”€ RAG/                 # RAG pipeline (retrieval + generation)
â”‚   â”œâ”€â”€ model/               # GPT-style transformer model
â”‚   â”œâ”€â”€ tokenizer/           # Custom tokenizer logic
â”‚   â””â”€â”€ inference_repo/      # Model loading, checkpoints, inference
â”‚   â”œâ”€â”€ app/                 # streamlit app
â”‚   â””â”€â”€ data/                # Medical document used for RAG
â”‚   
|    
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ fastapi.Dockerfile   # Backend container
â”‚   â””â”€â”€ streamlit.Dockerfile # Frontend container
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ tests/                   # Unit tests (Pytest)


âš™ï¸ Installation & Setup
1. Clone the Repository
git clone https://github.com/Albin78/LLM-Project.git
cd LLM-Project

2. Install Dependencies
pip install -r requirements.txt

3. Run Backend (FastAPI)
uvicorn src.api.app:app --host 0.0.0.0 --port 8000 --reload


API available at: ğŸ‘‰ http://localhost:8000/docs

4. Run Frontend (Streamlit)
cd frontend
streamlit run streamlit_app.py


Frontend available at: ğŸ‘‰ http://localhost:8501

ğŸ³ Docker Deployment

Build & run with Docker Compose:

docker-compose up --build


FastAPI â†’ http://localhost:8000

Streamlit â†’ http://localhost:8501

ğŸ” API Endpoints

POST /generate â†’ Response from GPT model

POST /rag-query â†’ Response with RAG pipeline

GET /health â†’ Health check

Docs: ğŸ‘‰ http://localhost:8000/docs

ğŸ“Š Workflow

User enters a medical query via Streamlit UI.

Query is sent to FastAPI backend.

RAG pipeline retrieves relevant documents from FAISS.

GPT-style model generates contextual response.

Answer is displayed in the frontend.

ğŸš€ Future Work

Fine-tuning with LoRA for efficiency.

Expanding RAG knowledge base with PubMed & guidelines.

Adding conversation memory for multi-turn queries.

CI/CD with GitHub Actions


âš ï¸ Disclaimer

This project is for research and educational purposes only.
It is not a substitute for professional medical advice.
Always consult a doctor for medical concerns.

